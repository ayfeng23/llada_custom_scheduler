{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71a4587b-4128-4a21-a204-7fbcee7491af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ayf4/.conda/envs/llada-env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer vocab size: 126349\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"GSAI-ML/LLaDA-8B-Instruct\", trust_remote_code=True)\n",
    "print(\"Tokenizer vocab size:\", len(tokenizer.get_vocab()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3bde284d-a57e-446d-af30-36e38c71d1b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /home/ayf4/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens in vocab: 126349\n",
      "Tokens that match real English words: 36342\n",
      "Percentage: 28.76%\n"
     ]
    }
   ],
   "source": [
    "#Comparing LLaDA Vocab with NTLK\n",
    "from transformers import AutoTokenizer\n",
    "import nltk\n",
    "from nltk.corpus import words\n",
    "import re\n",
    "\n",
    "# Download the dictionary\n",
    "nltk.download('words')\n",
    "english_words = set(words.words())\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"GSAI-ML/LLaDA-8B-Instruct\", trust_remote_code=True)\n",
    "\n",
    "# Access vocabulary\n",
    "vocab = tokenizer.get_vocab()\n",
    "tokens = list(vocab.keys())\n",
    "total_tokens = len(tokens)\n",
    "\n",
    "# Define a basic regex for word-like tokens (no special chars or digits)\n",
    "def is_potential_word(token):\n",
    "    clean = re.sub(r\"[^a-zA-Z]\", \"\", token)\n",
    "    return clean.lower() in english_words\n",
    "\n",
    "# Apply the check\n",
    "word_tokens = [t for t in tokens if is_potential_word(t)]\n",
    "num_word_tokens = len(word_tokens)\n",
    "percent = 100 * num_word_tokens / total_tokens\n",
    "\n",
    "print(f\"Total tokens in vocab: {total_tokens}\")\n",
    "print(f\"Tokens that match real English words: {num_word_tokens}\")\n",
    "print(f\"Percentage: {percent:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ef6cd972-6db9-4b42-a978-5d156a781adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /home/ayf4/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['meta', 'rider', 'åīļä¸º', 'twenty', 'åģłåĭ©', 'çi̇ħåħ³', 'eward', 'ä¹łæģ§', 'coe', 'é¢ģå¸ĥçļħ', 'å¤§ä½¿é¦ĩ', 'ç»¼åĳīå®ŀåĭľ', 'çľĭæ¸ħæ¥ļ', 'åıįé¦ī', 'indicted', 'ç¬¬åħ«ç«ł', 'filthy', '.**', 'å»·', 'capsule', 'é©±åĭ¨', '.check', 'åi̇łæĸĩåľ°åŀģ', 'sport', 'raising', 'åį°æľī', 'requent', 'projects', 'å¯¹æīĭ', 'æ·±å±±', 'ï¼įåĳ¸å¼ķäºĩ', 'æĭ½çĥł', 'å¤įåħ´', 'busters', 'peat', 'åħ³èģķæĸ¹', 'ted', 'è£ħ', '.shutdown', '-content', 'viewer', 'åı¯ä»¥ç»ļ', 'hdmi', 'jak', 'good', 'ç©¿è¡£æľį', 'captured', 'solving', 'toadd', 'åĳ½è¿ĳçļħ', 'complicated', 'æ¶¦æ»ĳåīĥ', 'ategories', 'pagination', 'ĉą', 'æĥĭåĸľ', 'çľĭçŀģå¥¹', 'æĭĳåī¶åīĥ', 'åīļå¤©', 'where', '-muslim', 'miserable', 'é¦ĭ', 'ä»£è¡¨æģ§çļħ', 'æĵĵè°i̇', '_scene', '.serializable', '+xml', 'parsons', 'inf', 'è®¢ç«ĭ', 'butt', 'gbp', 'vã¤', '(container', '_put', 'work', '±', 'å¿µå¿µ', 'å·¥ä½ľå²ĺä½į', 'åı¯è¢«', 'éľªçļħ', 'ãģģåĳīçĳĩ', 'fca', 'captain', 'nause', 'åºĳå±±', 'çļħä¸»è§ĵ', 'èģªæĺi̇', 'symbols', 'ãģģæķ¿æ²»', 'ï¼įè¯¢éĺ®', 'most', 'æ¿ģçĥī', 'åºķæķ¶è´¦æ¬¾', 'otechnology', \"('@\", 'vc', '_regs', 'lafayette']\n",
      "Total tokens in LLaDA vocab: 126349\n",
      "Tokens matching English words: 39790\n",
      "Percentage English words: 31.49%\n",
      "\n",
      "Sample of matching English words:\n",
      "['allege', 'horrific', 'novel', 'unethical', 'dup', 'ex', 'malt', 'fatigue', 'canonical', 'part', '_cam', 'mildly', 'âģĵthe', 'fox', '_prob', 'farm', 'po', 'truncated', 'modification', 'holl', 'pist', 'shipping', 'popular', '.save', 'response', '_document', 'react', 'avoid', 'revision', 'considering']\n"
     ]
    }
   ],
   "source": [
    "# Compare LLaDA Vocab with NLTK Accounting Strongly For Ġ\n",
    "from transformers import AutoTokenizer\n",
    "import nltk\n",
    "from nltk.corpus import words\n",
    "import re\n",
    "\n",
    "# Download English word list if not already available\n",
    "nltk.download('words')\n",
    "english_words = set(words.words())  # normalize all to lowercase\n",
    "\n",
    "# Load the LLaDA tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"GSAI-ML/LLaDA-8B-Instruct\", trust_remote_code=True)\n",
    "\n",
    "# Access raw token list\n",
    "vocab = tokenizer.get_vocab()\n",
    "tokens = list(vocab.keys())\n",
    "total_tokens = len(tokens)\n",
    "\n",
    "# Normalize tokens: strip leading Ġ and lowercase\n",
    "tokens_cleaned = [t.lstrip(\"Ġ\").lower() for t in tokens]\n",
    "\n",
    "# Define match logic: full alphabetic word and in English dictionary\n",
    "def is_potential_word(token):\n",
    "    return token.isalpha() and token in english_words\n",
    "\n",
    "def is_potential_word(token):\n",
    "    clean = re.sub(r\"[^a-zA-Z]\", \"\", token)\n",
    "    return clean.lower() in english_words\n",
    "\n",
    "# Apply filter\n",
    "word_tokens = [t for t in tokens_cleaned if is_potential_word(t)]\n",
    "num_word_tokens = len(word_tokens)\n",
    "percent = 100 * num_word_tokens / total_tokens\n",
    "\n",
    "# Output results\n",
    "print(f\"Total tokens in LLaDA vocab: {total_tokens}\")\n",
    "print(f\"Tokens matching English words: {num_word_tokens}\")\n",
    "print(f\"Percentage English words: {percent:.2f}%\")\n",
    "\n",
    "# Sample matching tokens (optional)\n",
    "import random\n",
    "print(\"\\nSample of matching English words:\")\n",
    "print(random.sample(word_tokens, 30))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "27d83a19-27fb-4066-ab86-a07dd1544bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "punctuation    :  2180 tokens (2.52%)\n",
      "digits         :    10 tokens (0.01%)\n",
      "code_like      :  1593 tokens (1.84%)\n",
      "subwords       :  2221 tokens (2.57%)\n",
      "foreign_like   : 19943 tokens (23.04%)\n",
      "symbols        :     0 tokens (0.00%)\n",
      "unknown        : 60612 tokens (70.02%)\n"
     ]
    }
   ],
   "source": [
    "#Categorizing Remaining LLaDA Tokens\n",
    "import re\n",
    "\n",
    "non_word_tokens = [t for t in tokens_cleaned if t not in word_tokens]\n",
    "\n",
    "# Categories to track\n",
    "categories = {\n",
    "    'punctuation': [],\n",
    "    'digits': [],\n",
    "    'code_like': [],\n",
    "    'subwords': [],\n",
    "    'foreign_like': [],\n",
    "    'symbols': [],\n",
    "    'unknown': [],\n",
    "}\n",
    "\n",
    "# Regex helpers\n",
    "is_digit = lambda t: re.fullmatch(r'\\d+', t)\n",
    "is_punct = lambda t: re.fullmatch(r'\\W+', t)\n",
    "is_code = lambda t: bool(re.search(r'[_<>/{};=]', t))\n",
    "is_symbol = lambda t: bool(re.fullmatch(r'[^\\w\\s]+', t))\n",
    "looks_foreign = lambda t: bool(re.search(r'[éöçñžあ語你]', t))  # crude but effective\n",
    "looks_subword = lambda t: re.match(r'^##|▁|^[a-z]{1,2}$', t)  # subword prefixes or short fragments\n",
    "\n",
    "# Categorize\n",
    "for token in non_word_tokens:\n",
    "    if is_digit(token):\n",
    "        categories['digits'].append(token)\n",
    "    elif is_punct(token):\n",
    "        categories['punctuation'].append(token)\n",
    "    elif is_code(token):\n",
    "        categories['code_like'].append(token)\n",
    "    elif is_symbol(token):\n",
    "        categories['symbols'].append(token)\n",
    "    elif looks_foreign(token):\n",
    "        categories['foreign_like'].append(token)\n",
    "    elif looks_subword(token):\n",
    "        categories['subwords'].append(token)\n",
    "    else:\n",
    "        categories['unknown'].append(token)\n",
    "\n",
    "# Print summary\n",
    "for cat, items in categories.items():\n",
    "    print(f\"{cat:15s}: {len(items):5d} tokens ({len(items)/len(non_word_tokens)*100:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fbbe53b8-310a-4cf6-9573-60df7bea4e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['åľłèģį', 'audited', 'rizz', 'îµ', 'æ¸ħåĩģ', 'arabic', 'validates', 'elsius', 'serviceimpl', 'ãģĳãģĳ', 'ï¼įæķ¾åħ¥', 'å°ģåłµ', 'atar', 'mis', 'looks', 'æľīæľ«', 'æ°¸', 'èĩ¼', '(html', 'æ·»åĭłåīĥ', 'è¡¨å§ĳ', 'ograph', 'leneck', 'payload', 'ï¼įè·¯ä¸ĭ', 'imo', 'mammals', 'alyzed', 'wednesdays', 'èµħæł¼èģĥè¯ķ', 'æīĳä»½', 'hhs', 'å°ĩæĺ¯', 'alid', 'åįķåħĥæµĭè¯ķ', '.stdout', 'ï¼įä½ĩåį´', 'eness', 'harmon', 'omnia', '\\\\leq', 'cular', 'suk', '.bytes', 'unve', 'employees', 'idation', 'queries', 'ãģģèįī', 'lian', 'orect', 'whatsapp', 'åįĭæīª', 'vonne', 'ffff', 'amelior', 'èĩ³å°ĳä¸ģä¸ª', 'mca', 'åĵīä½ľå¤§åń¦', 'ãģĥä»ĸè¯´', 'å£«æ°ķ', 'knobs', 'å¤©æ²³', 'kel', 'ä¿ŀå®ī', 'åń¦æľ¯äº¤æµģ', 'iran', 'cin', 'leb', 'æĭ½æł·', 'å¼ºåº¦åĵį', 'hering', 'aired', 'å°±å°ĩ', 'å¯¼èĩ´äºĩ', 'ssh', 'java', 'à¶', 'åľ¨æīĳèº«ä¸ĭ', 'å°±ä¸įèĥ½', 'æ»ĭåĳ³', '(src', 'ically', 'ï¼įåľ¨æīģè¿°', 'filepath', 'ï¼įä¸´', ',æīĳä»¬åľ¨', 'ï¼įä¸ģåıį', 'connecticut', 'ãģĥè¿ļåı¥è¯ŀ', 'â¦', 'ï¼įåĳħä¸ª', 'felix', 'xamarin', 'åĩłåįģä¸ĩ', 'benchmark', 'å½¢åĭ¿', 'devices', 'ï¼įåģļå¥½', 'omenclature', 'å®«ä¸»', 'bados', 'anded', 'åĳĥä¸ģäºľ', 'æķ¿', 'ble', 'qstring', 'ä¸ķåħ¶', 'plas', 'hipp', 'ï¼įåħ¨æĺ¯', '(comp', 'åľ¨åħ¶', 'latino', 'pods', 'æĳĩæķ¾', 'seized', 'ä¹ĭè°ľ', 'æ¬ł', 'circ', 'ï¼įä¼¤', 'ãģģæľ¬æ¬¡', 'åĩħèĵļåı¤', 'hal', 'æµ·è±ļ', 'äºĭåħī', 'ï¼įæīģä»¥å¥¹', 'æģªä¸įå¾ĺ', 'ielding', 'shores', 'æń£å®ĺ', 'ified', 'åĳįæ¬¾', 'è¢«å¥¹', 'ä¸īä¸ĥ', 'å¿§èļĳ', 'è¸ŀ', 'def', 'è°ª', 'èµħæł¼å®¡æł¥', 'egfr', 'renewed', 'compiling', 'securities', 'isempty', 'itect', 'cher', 'download', 'æµ·æ»©', 'upp', 'choices', 'kidnapped', 'ï¼įä½ĩæĺ¯åľ¨', 'æĺ¯åľ¨', 'publications', 'developing', 'ä¸ģäº®', 'daniels', 'muslim', 'rology', 'televised', 'uli', 'åľ¾èħ¾', 'æĵŀ', 'åıĥæķ°', '.linspace', 'æ¯ľåľĭ', 'å°±å¤łäºĩ', 'ppard', 'âģł', 'è¿ĺä¸įèĥ½', 'è¿ĩæ¸¡', 'uties', 'å¼ģæºĳ', 'å¥ķèµ°', 'ï¼įèµ¤', 'croatia', 'ä¸įæľ¾', 'infl', 'inton', 'è°ĥåĭ¨', 'åłºåŀĳ', 'è¥¿æ¸¸è®°', 'ä¸įèĥľæķ°', 'serbia', 'æ²ļ', 'palin', 'appcompat', 'åľºä½ĵ', 'è¨ģ', 'äºĩä¸ģæ¬¾', 'è¡¨è±¡', 'æĥķæģ¨', 'ä¼ĺåįĸ', 'inhab', 'å¡', 'æıĳä¾ľåħįè´¹', 'äººåĭľ', 'æŀģå¤§åľ°', 'á¿', 'logs', 'erial', 'chennai', 'advertisers', 'aur', '-lang', 'estion', 'perf', 'ï¼įå¥½åľ¨', 'å¹³åı°åĵį', 'åģļäºĭ', 'æºł', 'techn', 'ordering', 'å¤«æĸ¯åłº', 'æ¤ńåľĩ', 'åģľä¸ĭæŀ¥', 'interpreted', 'å¹²åĭ²', 'åģļä¸ģä¸ª', 'idx', 'åħ·æľīä»¥ä¸ĭ', 'lil', 'å²ĺ', 'itical', 'struct', 'hotspot', 'angelo', 'filtered', 'ä¼ģä¸ļåĳīå¹¶', 'htc', 'hotmail', 'åĭ¨ä¸įåĭ¨', 'å®½æķŀ', 'è¾©è®º', 'anc', 'converters', 'ãªncia', ',åıįæĸ¹', 'ä¸īå®¶', 'åħ´', 'åıĳåĭ¨æľº', 'ï¼įä¸įè®¸', 'inquiries', 'axies', 'rte', 'stri', 'guards', 'letting', 'å¢ŀåĭłåī°', 'è¿ŀè´¥', 'ata', 'korean', 'pcs', 'ãģģè¿ĳåĭ¨', 'caso', 'african', 'ä»£åĭŀ', 'ãģģåľ½å®¶', 'atham', 'åī¯æģ»è£ģ', 'åĳ¨è¾¹', 'enders', 'æ²¡æľīä»ģä¹ī', 'zeus', 'francis', 'factors', 'åī°ä½ł', 'milford', 'ä¿©', 'æĸ°äº¬', 'å', 'zew', '.execut', 'å¤ĸä¼¤', 'apis', 'æ²¡äºº', 'åįķåĩ»', 'è¿ļåī¯', 'skirts', 'deprecated', 'doubles', 'brian', 'estyles', 'stimulates', 'wells', 'è§ĥå¯łåī°', 'paral', 'pins', 'å½ĵæĺ¶', 'addon', 'ï¼įå¾ĺåī°', 'åģĩä½ĵ', 'ä¸ģæ»´', 'backend', 'è¾ľåĭ³', 'udos', 'spying', 'ä¸ģæ£µ', 'åľ°åºķ', 'elles', 'yte', 'ï¼įè·¯', 'gmail', 'epile', 'errors', 'licks', 'problems', 'polyg', '-oriented', 'jul', 'cout', 'facebooktwitter', 'æŀģæ°ķ', 'è¾ī', 'photoshop', ',æīĳæĺ¯', 'æĺ¥åń£', 'reciation', 'itter', 'apk', 'ï¼įæīĳè¯´', 'æĺ¯å¦ĥä½ķ', 'itational', 'å¼ķè¿ľäºĩ', 'ðŀ', 'carts', 'bugs', 'å¤´åĥı', 'highlands', 'åłºæľ¬ä¸ĭ', 'æł¡åľń', 'xia', '(int', 'è½¯æĸĩ', 'inc', 'usky', 'seaw', 'contains', 'approves', 'qui', 'ï¼įåį³åı¯', '.addall', 'æĸĩåįĸèīºæľ¯', 'recommends', 'reeves', 'å£ĳ', 'editions', 'åńķå¦ĩ', 'âģĭâģĭ', 'åīľä¸ļæŀ¿', 'analges', 'careers', 'åľłåľ°', 'è¢«æŀģ', 'è¯¶', 'meadows', 'cons', 'pitches', 'ãģģæľ±', 'airlines', 'arketing', 'rosine', 'modulated', 'kiev', 'åįģä¸ģå¹´', 'èĩ³', 'vich', 'æń¤äºº', 'ï¼įè´łè´£', 'åľŀæµģ', 'isible', '-ups', 'å°ıèıľ', 'lett', 'arts', 'ameda', 'ä¸¤ä»¶', 'benull', 'ï¼īåıĭ', 'åıĭè°ĭ', 'pecting', 'degrees', 'obj', 'raisins', 'å©µ', 'å²į', 'äºĩä½ł', 'æ²¡è¯´è¯ŀ', 'adol', 'æĭĺæĺ±', 'bible', 'eanor', 'urai', 'ä¸ģä¸ªæľī', 'staggered', 'rawing', '.co', 'dinners', 'è¯¥åī§', 'ancelled', 'ritt', 'ipv', 'ï¼įæīĺæĸĺ', 'å¿½', 'åĵīåĵīåĵī', 'retro', 'ï¼įåľłä¸ºæīĳ', 'ãģĥåıªæľī', 'ä¸ģä¸ĸ', 'æķļä¼ļ', 'ights', 'ï¼įå°ĳ', 'åįĥå¤ľ', 'ued', 'resources', 'teleport', 'igin', 'fluct', 'åºķåńĳ', 'sul', 'https', 'breaths', 'è¯¸å¤ļ', 'olphin', 'åī°è´¦', 'èľ»', 'alloc', 'ä¸įå¾ĺäºĩ', 'arranged', 'mirrors', 'è¯´æīĳä»¬', 'ï¼įæ·±åħ¥', ',åīĩ', 'agro', 'å¨ĵå¨ĵ', 'sony', 'ethod', 'ensued', 'automation', 'æĺ¯ä¸ªå¥½', 'æīĳåı¯', 'åįºåłłæģ§', 'meme', 'avage', ',èģįæĺ¯', 'tumblr', 'validationerror', 'ungsten', 'äºįåńĺ', 'intercepted', 'titles', 'ä¸ģåıį', 'perce', 'viewport', 'åľ½', 'è±¡æ£ĭ', 'greeks', 'fmt', ',åĳ´', 'èīĩ', 'åľĭæĭ¬', 'empre', 'pos', 'roman', 'åĥ¬åįĸåīĥ', 'guitars', 'ï¼įæĺ¯å¯¹', 'alger', 'creates', 'improvements', 'ques', 'engagements', 'æµĳæµĭ', 'ï¼įæĺ¯è¦ģ', 'saints', 'æľ¹æĵį', 'pharmacists', 'aternity', 'å´ķ', 'stringbuilder', 'argumentexception', 'ä¼łè¯´ä¸ń', 'ä¸ĵä¸ļåįĸ', 'æľģå°ĳ', 'cfp', 'å°ı', 'ãģģå¢ŀ', 'ä¸ģåĩį', 'username', 'å¾īä¹ħä»¥åīį', 'èĥģ', 'æ¶ĥä¸ĭ', 'dwight', 'callbacks', 'æľīåīĭ', 'å¨ĺå®¶', 'èĥ½èģĺ', 'bial', 'ension']\n"
     ]
    }
   ],
   "source": [
    "#Sampling remaining tokens\n",
    "import random\n",
    "print(random.sample(categories['unknown'], 500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d88943f-22a8-4f00-9ace-e75ae6319f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean ASCII-like tokens: 15595\n",
      "Percent of unknowns that are ASCII-like: 24.11%\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# This regex allows only ASCII letters, numbers, and common symbols.\n",
    "ascii_clean_pattern = re.compile(r'^[a-zA-Z0-9\\-\\._\\+\\*\\?!@#%&=\\[\\]{}()/\\\\|,:;\"\\'<>\\^`~]+$')\n",
    "\n",
    "ascii_like = [t for t in categories['unknown'] if ascii_clean_pattern.fullmatch(t)]\n",
    "ascii_like_pct = len(ascii_like) / len(categories['unknown']) * 100\n",
    "\n",
    "print(f\"Clean ASCII-like tokens: {len(ascii_like)}\")\n",
    "print(f\"Percent of unknowns that are ASCII-like: {ascii_like_pct:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f7fdcb0-19e0-4d77-b853-48b92656f876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- ASCII-Like vs Non-ASCII ---\n",
      "ASCII-like non-word tokens   : 19089 (21.21%)\n",
      "Non-ASCII or corrupted tokens: 70918 (78.79%)\n"
     ]
    }
   ],
   "source": [
    "# Define what counts as \"ASCII-clean\": only standard letters, digits, and common symbols\n",
    "ascii_clean_pattern = re.compile(r'^[a-zA-Z0-9\\-\\._\\+\\*\\?!@#%&=\\[\\]{}()/\\\\|,:;\"\\'<>\\^`~]+$')\n",
    "\n",
    "ascii_like = []\n",
    "non_ascii_like = []\n",
    "\n",
    "for token in non_word_tokens:\n",
    "    if ascii_clean_pattern.fullmatch(token):\n",
    "        ascii_like.append(token)\n",
    "    else:\n",
    "        non_ascii_like.append(token)\n",
    "\n",
    "# Report\n",
    "ascii_like_count = len(ascii_like)\n",
    "non_ascii_like_count = len(non_ascii_like)\n",
    "ascii_percent = 100 * ascii_like_count / len(non_word_tokens)\n",
    "\n",
    "print(f\"\\n--- ASCII-Like vs Non-ASCII ---\")\n",
    "print(f\"ASCII-like non-word tokens   : {ascii_like_count:5d} ({ascii_percent:.2f}%)\")\n",
    "print(f\"Non-ASCII or corrupted tokens: {non_ascii_like_count:5d} ({100 - ascii_percent:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "daa95ac8-4c2a-44fb-be5d-e469270be918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLaDA vocab tokens total        : 126349\n",
      "Tokens found in wiki_tfidf     : 14028\n",
      "Percentage of LLaDA vocab in wiki IDF set: 11.10%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the IDF dataset\n",
    "idf_df = pd.read_csv(\"wiki_tfidf_terms.csv\")  # Adjust path as needed\n",
    "idf_terms = set(idf_df['token'])\n",
    "\n",
    "# Load LLaDA tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"GSAI-ML/LLaDA-8B-Instruct\", trust_remote_code=True)\n",
    "llada_vocab = tokenizer.get_vocab()\n",
    "llada_tokens = list(llada_vocab.keys())\n",
    "\n",
    "# Compute overlap\n",
    "tokens_in_wiki = [t for t in llada_tokens if t in idf_terms]\n",
    "percent_overlap = 100 * len(tokens_in_wiki) / len(llada_tokens)\n",
    "\n",
    "# Output result\n",
    "print(f\"LLaDA vocab tokens total        : {len(llada_tokens)}\")\n",
    "print(f\"Tokens found in wiki_tfidf     : {len(tokens_in_wiki)}\")\n",
    "print(f\"Percentage of LLaDA vocab in wiki IDF set: {percent_overlap:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eb8e3206-d878-4449-8c67-7268b5348377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLaDA vocab tokens total        : 126349\n",
      "Tokens found in wiki_tfidf     : 60371\n",
      "Percentage of LLaDA vocab in wiki IDF set: 47.78%\n",
      "\n",
      "Sample of LLaDA vocab tokens NOT found in Wikipedia TF-IDF set:\n",
      "['ãģĥè¿ļéĩįçļħ', 'çĥģ', 'éŀłçŀģ', 'è¾ĥå¼º', '(elem', 'ï¼įä¸įè¦ģ', 'è¾ĵçķµ', 'æķ°æį®åńĺåĥ¨', '{u', 'ambiã©n', 'å®ŀåľ°èģĥå¯ł', 'ivable', 'bulld', 'irrit', 'æĭĩéļ¤', '.junit', 'åĩħå¤ĸ', 'ãģĥå®ĥæĺ¯', '|=', 'æķ£åıĳåĩº', 'ãģģä¿ħç½ĺæĸ¯', 'ä¸ńå¸¦', 'invalidoperationexception', '_process', 'ä¼ļéģīæĭ©', 'å¹¿å¤§', 'å¿«éģłå¢ŀéķ¿', 'çķµæºĳ', 'æ¯ķè¾ĥé«ĺçļħ', '++', 'ä¿®é¥°', 'ç¡¬çľĺ', 'âģļl', 'çļħç²īä¸ŀ', 'ï¼įåľłä¸ºæīĳä»¬', 'èĳ¡èĳħç³ĸ', 'æłĳä¸ģ', 'å®¶è£ħ', '.csdn', 'çļħäºĭçī©', 'åı£æ°´', 'èī¹èī±', 'çļħé£i̇éļ©', 'è¯įæŀ¡', 'ä¹łåıªæľī', 'ç»§ç»ńè¯´éģĵ', '_msk', 'èĥ½ä½¿', 'orrow', 'ç»į']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "import random\n",
    "\n",
    "# Load the Wikipedia TF-IDF dataset\n",
    "idf_df = pd.read_csv(\"wiki_tfidf_terms.csv\")\n",
    "#idf_terms = set(t.lower() for t in idf_df['token'])  # lowercase all TF-IDF terms\n",
    "idf_terms = set(str(t).lower() for t in idf_df['token'].dropna())\n",
    "\n",
    "\n",
    "# Load LLaDA tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"GSAI-ML/LLaDA-8B-Instruct\", trust_remote_code=True)\n",
    "llada_vocab = tokenizer.get_vocab()\n",
    "llada_tokens = list(llada_vocab.keys())\n",
    "\n",
    "# Normalize LLaDA tokens: strip leading Ġ and lowercase\n",
    "llada_tokens_cleaned = [t.lstrip(\"Ġ\").lower() for t in llada_tokens]\n",
    "\n",
    "# Compute overlap\n",
    "tokens_in_wiki = [t for t in llada_tokens_cleaned if t in idf_terms]\n",
    "percent_overlap = 100 * len(tokens_in_wiki) / len(llada_tokens_cleaned)\n",
    "\n",
    "# Output\n",
    "print(f\"LLaDA vocab tokens total        : {len(llada_tokens_cleaned)}\")\n",
    "print(f\"Tokens found in wiki_tfidf     : {len(tokens_in_wiki)}\")\n",
    "print(f\"Percentage of LLaDA vocab in wiki IDF set: {percent_overlap:.2f}%\")\n",
    "\n",
    "# Sample tokens not in Wikipedia\n",
    "tokens_not_in_wiki = [t for t in llada_tokens_cleaned if t not in idf_terms]\n",
    "print(\"\\nSample of LLaDA vocab tokens NOT found in Wikipedia TF-IDF set:\")\n",
    "print(random.sample(tokens_not_in_wiki, 50))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "43999b01-00c9-42b2-b444-92beaa27d058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample of LLaDA vocab tokens NOT found in Wikipedia TF-IDF set:\n",
      "(\"*\n",
      "ãģģéĥĵ\n",
      "ï¼įä¼ģåľ¾\n",
      ",åģ¼å¾ĺ\n",
      "æĺł\n",
      "çľĭè§ģ\n",
      "ä¸ńæıĳåıĸ\n",
      "è¾ĵäºĩ\n",
      "×ľ\n",
      "æĭķ\n",
      "ãģģåĳħç§į\n",
      "è¯į\n",
      ",çńī\n",
      "çī©ä»·\n",
      "çªģå¦ĥåħ¶æŀ¥çļħ\n",
      "(;\n",
      "icions\n",
      "åľ¨åħ¶ä»ĸ\n",
      "aluronic\n",
      "è£ģå®ļ\n",
      "ç»¼åĳī\n",
      "æŀħçńĳ\n",
      "(\"<\n",
      "æķħäºĭæĥħèĭĥ\n",
      "olysis\n",
      "electroph\n",
      "å¯¹çħ§\n",
      "ç¨ļå«©\n",
      "åı¤èģģçļħ\n",
      "æīĳå°±\n",
      "éģıå½»\n",
      ".cod\n",
      "ä¸»äººåħ¬\n",
      "æĺ©å°±\n",
      "çī©ä¸ļ\n",
      "å¤§åį«\n",
      "ä¸įåī°\n",
      "åi̇ĭåĭľåĵį\n",
      "æĸ°åħ´\n",
      "èī¯å¤ļ\n",
      "à°¿\n",
      "ï¼įèħ±\n",
      "ãģģçļ®èĥ¤\n",
      "åħ±äº§ä¸»ä¹ī\n",
      "é»ħ\n",
      "_screen\n",
      "è¢«æĭķèµħ\n",
      "åįķåħĥæµĭè¯ķ\n",
      "è´¯å½»\n",
      "å¼łæľľ\n",
      "è®°äºĭ\n",
      "(ed\n",
      "\"\"\"\n",
      "ä¸¾ä¸ª\n",
      "_impl\n",
      "æľ´æĸ°æĺ¶éĺ´\n",
      "é«ĺåħ´çļħ\n",
      "çļħåĵį\n",
      "èººåľ¨åľ°ä¸ĭ\n",
      "çīĩåıĳçļħ\n",
      ",æ¿ģåıĳ\n",
      "ä¸»äººçļħ\n",
      "ï¼įéº»çĥ¦\n",
      "ä¸i̇ç¬¬äºį\n",
      "å°ıç»ħ\n",
      "ä¸ńåĳ«\n",
      "ä½ļ\n",
      "æŀľæłĳ\n",
      "åīļè¦ģ\n",
      "ï¼įåī¶\n",
      "($\"\n",
      "ä½ļ\n",
      "reated\n",
      "ä¸ģåīļ\n",
      "çńīåľ°çļħ\n",
      "ä½įç½®ä¿¡æģ¯\n",
      "æ²¾æłĵ\n",
      "æ²ļæ¼ł\n",
      "unexpectedeof\n",
      "ãģģæĭĺ\n",
      "æĺł\n",
      "lemented\n",
      "åĩºä¸ģåī¯\n",
      "\\]).\n",
      "è¯´å®į\n",
      "ãģģæīĭæľº\n",
      ".ext\n",
      "æĺ¯ä¼ģä¸ļ\n",
      "ïģî¹\n",
      "äºįäºº\n",
      "åĩºçīīæĺ¶éĺ´\n",
      "è®¾æ³ķ\n",
      "_load\n",
      "æi̇¥çķµè¯ŀ\n",
      "åį´åıĳçi̇°\n",
      "ä½ıæī·\n",
      "åi̇¿äººæ°ĳæķ¿åºľ\n",
      "ä½łå°±æĺ¯\n",
      "æŀ¸\n",
      "<u\n",
      "ãģĥç»¼åĳī\n",
      "å®ĺåºľ\n",
      "äºķæ°´\n",
      "ä¸ģè§ī\n",
      "éľ·\n",
      "æ¸ļ\n",
      "é£ŀé¸ł\n",
      "éķļè¿ĩ\n",
      "_stack\n",
      "æĥ³è®©\n",
      "jpanel\n",
      "ä¸ģè¯ń\n",
      "\n",
      "å¤§åĸŀ\n",
      "åıĳå±ķ\n",
      "äºĩè¿ļäºľ\n",
      "ç§ĭæ°´\n",
      "æĺ¯åi̇»\n",
      "{s\n",
      "ï¼įåħīæĺ¯\n",
      "æľµ\n",
      "_no\n",
      "'][$\n",
      "ãģģè¿ŀ\n",
      "å¼łå¤§\n",
      "å±ħä½ıè¯ģ\n",
      "æĥ³è±¡ä¸ń\n",
      "ï¼įåľłèģį\n",
      "ãģĥéĵīå¯¹\n",
      "æļ¯èī²\n",
      "æ¸ħæ·¡\n",
      "ï¼įåįº\n",
      "åĵīåī©\n",
      "ð½ñĭðµ\n",
      "ä¹łä¸įå¿ħ\n",
      "nan\n",
      "ä¼łç»ł\n",
      ".hash\n",
      "ø¶\n",
      "é£i̇çķµ\n",
      "ï¼įåı¯æĺ¯å¥¹\n",
      "è¯ļæģ³\n",
      "å°±åĥı\n",
      "ï¼įæģ§ä»·æ¯ķ\n",
      "(on\n",
      "ï¼įç«¯\n",
      "éļīåīŀ\n",
      "æ¼¾\n",
      "å°ĳ\n",
      "æħīåıĳ\n",
      "çļħèµħæĸļ\n",
      "ä¿ŀå®ī\n",
      "_trans\n",
      "è§ģä¹ł\n",
      "éģģä¼ĳ\n",
      "èĥħ\n",
      "åħ¨çľģ\n",
      "éĥ½ä¸º\n",
      "æ·ĺæ±°èµľ\n",
      "((\n",
      "ãģģæķ¿æ²»\n",
      "æįºæĭķ\n",
      "è½»æĺĵåľ°\n",
      "æĸĩä»¶å¤¹\n",
      "_phase\n",
      "ïĥî·\n",
      "å¯¼åĳĳ\n",
      "atform\n",
      "ä¼ģ\n",
      "(/\n",
      "è½¬æĭĺ\n",
      "å¥³åń©çļħ\n",
      "æī¶æįģ\n",
      "ç£ĭåķĩ\n",
      "=\"../../../../../../\n",
      "ä¸¤æł¹\n",
      "ä¸½èi̇i̇çļ½\n",
      "åħ¨éĥ¨éĥ½\n",
      "æĸĭ\n",
      "æī¾äºĩ\n",
      "_android\n",
      "æıĵæīĭ\n",
      "æµ¦åįº\n",
      "åĭ¨æ¤įçī©\n",
      "æľīä¸ģèĥ¡\n",
      "ï¼įè¾ľ\n",
      "inated\n",
      "_byte\n",
      "æ´»è¡ģ\n",
      "zzarella\n",
      "<|reserved_token_94|>\n",
      "å¹²åĳĺ\n",
      "ï¼įåľ¨ä½ł\n",
      "åŀıäºĭ\n",
      "é£ŀæľºä¸ĭ\n",
      "å¥½äºľ\n",
      "ãģĥä¸i̇æń¤åĳįæĺ¶\n",
      "æĺł\n",
      "ã³l\n",
      "æķ¹åıĺçļħ\n",
      ".findall\n",
      "èµ¤\n",
      "çºµåĳĳ\n",
      "æķ¿æ²»ç«ļä½į\n",
      "è¾ľè¾ľèĭ¦èĭ¦\n",
      ",å»ºç«ĭ\n",
      "ï¼įä½ĩéļıçŀģ\n",
      "å¾ļ\n",
      "çķ¨çļħ\n",
      "åĳĳæīģè¿°\n",
      "èĭ±é¦ļ\n",
      "æģ»éĥ¨\n",
      "î¿\n",
      "è±ģ\n",
      "ä¸ģèµ°\n",
      "çķľèľľ\n",
      "ç»łè®¡åń¦\n",
      "æī¾åī°äºĩ\n",
      "å±ħä½ı\n",
      "ç§ģäºº\n",
      "imachinery\n",
      "ï¼įåįĺåįĺ\n",
      "ãģģåı¯\n",
      "æ³¢éķ¿\n",
      "äºįåįģå¤ļ\n",
      "éĺµèĳ¥\n",
      "å¼ģå¾ģ\n",
      "ç»ĩèĩ´\n",
      "çķłæ°ķçļħ\n",
      "{j\n",
      "(debug\n",
      "éľ¨éľª\n",
      "åĭŀåħ¬å®¤çļħ\n",
      "_(\n",
      "åħ¶çħ¶\n",
      "äºĩä¸ģä¸ŀ\n",
      "keyvalue\n",
      "ï¼įä¸ģåıį\n",
      "_tot\n",
      "çĥ¹äºĩ\n",
      "éĵ®\n",
      "ä»ĸä»¬å¯¹\n",
      "ãģĥä¸ªäºº\n",
      "åľ¨çi̇°åľº\n",
      "åıīä¸įæĺ¯\n",
      "_man\n",
      "ï¼įæĺłè®º\n",
      "çļħè¯ŀå°±\n",
      "æ¸©å·®\n",
      ",èĩªçħ¶\n",
      "(next\n",
      "åĳ\n",
      "apore\n",
      "è¯ńçļħ\n",
      "ï¼įéľĩ\n",
      "ï¼įä¸ºæĥ¨\n",
      "ä¸½é¢ĸ\n",
      ",éĥ£å°±\n",
      "ç¥ŀåħ½\n",
      "ä¼ļåĵį\n",
      "\"));\n",
      "ï¼įæįīçħ§\n",
      "çĸ£\n",
      "æĸ¯æ´ľ\n",
      "è¾ĥå°ĳ\n",
      "æĸ¹çļħ\n",
      "ãģģçī±\n",
      "ï¼įåıªè¦ģä½ł\n",
      "èģļæ°¨éħ¯\n",
      "éĩįå¤§é¡¹çľ®\n",
      "æ¥ļ\n",
      "åĳĭéķģ\n",
      "éļįçķł\n",
      "å®ŀä½ĵåºĺ\n",
      "æĭ½ç©º\n",
      "utenant\n",
      "ï¼įå°ĩæŀ¥\n",
      "åį°æľī\n",
      "éĥĭ\n",
      "å»ºçńĳéŀ¢ç§¯\n",
      "åľľåīĩä¹ĭä¸ģ\n",
      "æ··æŀĥ\n",
      "ï¼įè¾ĵåħ¥\n",
      "ç®ģçľ´å°±æĺ¯\n",
      "è¦ģå¦ĥä½ķ\n",
      "å»ºç«ĭçļħ\n",
      "èĳ¥ä¸ļæķ¶åħ¥\n",
      "å¤¹æŀĥçŀģ\n",
      "åį±éļ©äºĩ\n",
      "éĺ²\n",
      "åĩ½\n",
      "åįħæĭ¬ç¬¬ä¸ģ\n",
      "åįħæīi̇\n",
      "ð²ñĭ\n",
      "ä¸ģçº¢\n",
      "åĩºä»·\n",
      ".reference\n",
      "çļħä¸ģå£°\n",
      "åĳi̇ä¸įä¹ħ\n",
      "umni\n",
      "æi̇łè¿ĩ\n",
      "éľģè°¨æħi̇\n",
      "çļħè¯·æ±ĥ\n",
      "æħi̇\n",
      ".bool\n",
      "îķ\n",
      "åľ´æķ»\n",
      "ä¸ĭåńĳ\n",
      "æīģè°ĵçļħ\n",
      "èīľ\n",
      "åĩļçķł\n",
      ".builder\n",
      "ð²ðĥ\n",
      "/*\n",
      "å¥ĩ\n",
      "ç»ĵæŀłåĳi̇\n",
      "æĺłè®º\n",
      "å¯¹æĸ¹\n",
      "èħĳçĥĥ\n",
      "åıīåĳį\n",
      "åįĺå¤§\n",
      "æīĭæŀª\n",
      "âī¤\n",
      "è¦ı\n",
      "}}^{\\\n",
      "æi̇§åī¶\n",
      "_arch\n",
      "è¿ľåľ¨\n",
      "è®¹\n",
      "iography\n",
      "ï¼įè¿ĳåĭ¨\n",
      "çi̇ħ\n",
      "imerick\n",
      "æľ¬åľºæ¯ķèµľ\n",
      "éį\n",
      "çķ·åńĳçļħ\n",
      "çķľçī§ä¸ļ\n",
      "(-\n",
      ".dis\n",
      "åįĺè·¯\n",
      "è¿ķ\n",
      "åħ³æ³¨çļħ\n",
      "æļ®éģļ\n",
      "'\\\\\n",
      "ä½łè¿ļä¸ª\n",
      "æĺ¯å¥¹çļħ\n",
      "ç»ļå°ı\n",
      "è¿ĺçľł\n",
      "ï¼įæīĳæĭĭ\n",
      "åģ¾åĳĳäºi̇\n",
      "ä¸ĥå¤ķ\n",
      "ï¼įç®ģçľ´å°±æĺ¯\n",
      "çļħä¸ī\n",
      "çĥi̇çĥi̇\n",
      "æĥ³åĭŀæ³ķ\n",
      "æīĳæ²¡æľī\n",
      "ä¼¯æł¼\n",
      "ï¼įéĥ½æĺ¯\n",
      "è¸ıä¸ĭäºĩ\n",
      "èi̇«\n",
      "ãģģå®ī\n",
      "ä¸i̇è®¾è®¡\n",
      "èĥļèĥi̇\n",
      "æķı\n",
      "å¥¹ä¼ļ\n",
      "æįª\n",
      ".generic\n",
      "å®ŀéªįåń¦æł¡\n",
      "ãģĥåľ¨ä»ĸ\n",
      "çķ¨å°½\n",
      "éĥĳå·ŀ\n",
      "åħńä¸ªæľī\n",
      "æ´ĵèħ±\n",
      "-----\n",
      "(filepath\n",
      "ï¼įè¿ļæł·çļħ\n",
      "çļħçķłäº§\n",
      "è°ĥè¯ķ\n",
      "($\n",
      "æķ¾åľŀ\n",
      "viewholder\n",
      "æ¸ħéĩĵ\n",
      "éĺ¨åħ³\n",
      "ãĥ¬\n",
      "(set\n",
      "èľī\n",
      "æĥ¨éĩį\n",
      "è½½ä½ĵ\n",
      "çĺļ\n",
      "ï¼įè¦ģæĥ³\n",
      "corticoster\n",
      ".month\n",
      "å¹³è¡¡\n",
      "è¿ļåı¥è¯ŀ\n",
      "æ¸©åº¦ä¸º\n",
      "åįĥåħĥ\n",
      "æķ¯æµģ\n",
      "ã¥r\n",
      "à³\n",
      "è§£åi̇ĭ\n",
      "æ¥ļæ¥ļ\n",
      "æ¡ĥåľń\n",
      "åķ¯ä¸ģä¸ģä¸ª\n",
      "ã¤\n",
      "å¤ļæĺ¶\n",
      "ä¸ĵçķ¨çļħ\n",
      "ç»ļäºĩæīĳ\n",
      "å£ĳ\n",
      "ç¤¼è®°\n",
      "â·å¾·\n",
      "ï¼įæĭ¥\n",
      ",çi̇°åľ¨çļħ\n",
      "mathtt\n",
      "_o\n",
      "èº«ä»½çļħ\n",
      "å¾ģæŀ¥\n",
      "å°½æĥħ\n",
      ".fetch\n",
      "è½²\n",
      "çªģ\n",
      "ä½³ç»©\n",
      "é¼łæłĩ\n",
      "ä¼ĭå§ĭ\n",
      "å°ķæĸ¯\n",
      "å¯¹èģķ\n",
      "å¿ħé¡»çļħ\n",
      "çľĭä¸į\n",
      "_success\n",
      "ãģĥåīļ\n",
      "åįģåľľæŀ¡\n",
      ".modules\n",
      "æīģ\n",
      "éľģè¦ģ\n",
      "ç¤ºæħı\n",
      "åłi̇åįĺ\n",
      "ä¸¤ä¸ĭ\n",
      "èĥĸ\n",
      "ç²¾é«ĵ\n",
      "ä½ĵç³»çļħ\n",
      "æ®¿åłĥ\n",
      "æĸĳé©³\n",
      "å·´åłº\n",
      "âģĵ\n",
      "ð»ð¸\n",
      "ä¿ŀéĩį\n",
      "ãģĥæ¯ıæ¬¡\n",
      "èľľ\n",
      "è¦ģæł¹æį®\n",
      "ophobia\n",
      "ç¼´èi̇·\n",
      "éħĵæŀ¯\n",
      "ï¼įç«ĭåī»\n",
      "åī¶ä½ľ\n",
      "ä¹ĭç¾i̇\n",
      "ä¸¤ç§į\n",
      "åľ¨èº«ä¸ĭ\n",
      "é¾ļçi̇ĭ\n",
      "-nav\n",
      ".control\n",
      "è·³æ°´\n",
      "æi̇§ä»¶\n",
      "èµĺè¿°\n",
      "å¤¯\n",
      "è®¤ä¸ºæĺ¯\n",
      "\\|\\\n",
      "æīĳæĥ³è¦ģ\n",
      "ï¼īéļ¤\n",
      "èģ½æĳģ\n",
      "æĺħæ¸¸å±ģ\n",
      "æ¯ķä»ĸ\n",
      ".globalization\n",
      "çļħäººæŀ¥\n",
      "ä¸ńåń¦\n",
      "addobject\n",
      "_ram\n",
      "åĳī\n",
      "âīĩ\n",
      "ä¸ĭæŀ¥çļħ\n",
      "è¿ļä¸ªæł·åńĳ\n",
      "è¦ģç´§\n",
      "çi̇ĭå¦ĥ\n",
      "*</\n",
      "è¯ńéł³\n",
      "ç¯ĩç«ł\n",
      "åºķå±ĥ\n",
      "æŀ¡çłģ\n",
      "éļĳä½įåŀĺ\n",
      "$('\n",
      "facebookshare\n",
      "âģł\n",
      "çķ¨ä¸įçŀģ\n",
      "ãģģä¸ķ\n",
      "æ»ĳéľª\n",
      "åļi̇\n",
      "å¾ģäºĭ\n",
      "æľģè¿ĳ\n",
      "pyobject\n",
      "çńīçļħ\n",
      "åīĩç£ĭ\n",
      "ï¼įå¥½å¥½\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "# Sample 50\n",
    "sample_outliers = random.sample(tokens_not_in_wiki, 500)\n",
    "\n",
    "print(\"\\nSample of LLaDA vocab tokens NOT found in Wikipedia TF-IDF set:\")\n",
    "for token in sample_outliers:\n",
    "    print(token)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (llada-env)",
   "language": "python",
   "name": "llada-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
